{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LIBERO Pro 评估 Jupyter Notebook\n",
        "\n",
        "这个 notebook 允许你分步执行 LIBERO Pro 评估，支持灵活的参数配置。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: 导入库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11111111111111111111111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
            "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
            "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /hdd/zijianwang/openpi/examples/libero/.venv/lib/python3.8/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
            "[Open3D INFO] WebRTC GUI backend enabled.\n",
            "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
            "Warning: make sure gym is installed if you want to use the GymWrapper.\n",
            "✓ All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/hdd/zijianwang/openpi/third_party/LIBERO-PRO\")\n",
        "import collections\n",
        "import dataclasses\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import perturbation\n",
        "import tqdm\n",
        "import yaml\n",
        "from PIL import Image\n",
        "\n",
        "from libero.libero import benchmark\n",
        "from libero.libero import get_libero_path\n",
        "from libero.libero.envs import OffScreenRenderEnv\n",
        "from openpi_client import image_tools\n",
        "from openpi_client import websocket_client_policy as _websocket_client_policy\n",
        "\n",
        "from util import compute_eef_trajectory_from_actions, build_reusable_value_map, evaluate_trajectory_with_value_map\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: 定义常量和辅助函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Constants and helper functions defined\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "LIBERO_DUMMY_ACTION = [0.0] * 6 + [-1.0]\n",
        "LIBERO_ENV_RESOLUTION = 256  # resolution used to render training data\n",
        "\n",
        "# Helper functions\n",
        "def _quat2axisangle(quat):\n",
        "    # clip quaternion\n",
        "    if quat[3] > 1.0:\n",
        "        quat[3] = 1.0\n",
        "    elif quat[3] < -1.0:\n",
        "        quat[3] = -1.0\n",
        "\n",
        "    den = np.sqrt(1.0 - quat[3] * quat[3])\n",
        "    if math.isclose(den, 0.0):\n",
        "        return np.zeros(3)\n",
        "\n",
        "    return (quat[:3] * 2.0 * math.acos(quat[3])) / den\n",
        "\n",
        "\n",
        "def _get_libero_env(task, resolution, seed):\n",
        "    task_description = task.language\n",
        "    task_bddl_file = pathlib.Path(get_libero_path(\"bddl_files\")) / task.problem_folder / task.bddl_file\n",
        "    env_args = {\"bddl_file_name\": task_bddl_file, \"camera_heights\": resolution, \"camera_widths\": resolution}\n",
        "    env = OffScreenRenderEnv(**env_args)\n",
        "    env.seed(seed)\n",
        "    return env, task_description\n",
        "\n",
        "print(\"✓ Constants and helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: 定义日志和配置管理函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Logging and config management functions defined\n"
          ]
        }
      ],
      "source": [
        "def setup_logging(args):\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_id = f\"LIBERO-PRO-{args['task_suite_name']}-{timestamp}\"\n",
        "    if args.get('run_id_note') is not None:\n",
        "        run_id += f\"-{args['run_id_note']}\"\n",
        "\n",
        "    os.makedirs(args['local_log_dir'], exist_ok=True)\n",
        "    log_filepath = os.path.join(args['local_log_dir'], f\"{run_id}.txt\")\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "        handlers=[logging.StreamHandler(), logging.FileHandler(log_filepath, encoding=\"utf-8\")],\n",
        "    )\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(f\"Experiment run ID: {run_id}\")\n",
        "    logger.info(f\"Log file path: {log_filepath}\")\n",
        "\n",
        "    return logger, run_id, log_filepath\n",
        "\n",
        "\n",
        "def save_experiment_config(args, run_id, log_filepath):\n",
        "    if not args.get('save_experiment_config', True):\n",
        "        return\n",
        "\n",
        "    config_data = {\n",
        "        \"run_id\": run_id,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"args\": args,\n",
        "        \"log_file\": log_filepath,\n",
        "    }\n",
        "\n",
        "    config_filepath = os.path.join(args['local_log_dir'], f\"{run_id}_config.json\")\n",
        "    with open(config_filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    logging.info(f\"Experiment configuration saved to: {config_filepath}\")\n",
        "\n",
        "\n",
        "def save_episode_video(replay_images, task_description, episode_idx, success, run_id, args):\n",
        "    task_segment = task_description.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
        "    task_video_dir = os.path.join(args['video_out_path'], run_id, task_segment)\n",
        "    os.makedirs(task_video_dir, exist_ok=True)\n",
        "\n",
        "    suffix = \"success\" if success else \"failure\"\n",
        "    video_filename = f\"episode_{episode_idx:03d}_{suffix}.mp4\"\n",
        "    video_filepath = os.path.join(task_video_dir, video_filename)\n",
        "\n",
        "    try:\n",
        "        imageio.mimwrite(\n",
        "            video_filepath,\n",
        "            [np.asarray(x) for x in replay_images],\n",
        "            fps=10,\n",
        "        )\n",
        "        logging.info(f\"Episode video saved: {video_filepath}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save episode video: {e}\")\n",
        "\n",
        "print(\"✓ Logging and config management functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: 设置超参数\n",
        "\n",
        "在这个 Cell 中修改超参数以调整评估配置。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Hyperparameters configured:\n",
            "  host: 0.0.0.0\n",
            "  port: 8001\n",
            "  resize_size: 224\n",
            "  replan_steps: 5\n",
            "  sampling_bs: 8\n",
            "  task_suite_name: libero_spatial_displacement\n",
            "  num_steps_wait: 10\n",
            "  num_trials_per_task: 1\n",
            "  evaluation_config_path: /hdd/zijianwang/openpi/third_party/LIBERO-PRO/evaluation_config.yaml\n",
            "  local_log_dir: ./experiments/logs\n",
            "  run_id_note: None\n",
            "  save_experiment_config: True\n",
            "  video_out_path: ./experiments/videos/\n",
            "  seed: 7\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Model server parameters\n",
        "# ============================================================================\n",
        "host = \"0.0.0.0\"\n",
        "port = 8001\n",
        "resize_size = 224\n",
        "replan_steps = 5\n",
        "sampling_bs = 8\n",
        "\n",
        "# ============================================================================\n",
        "# LIBERO environment-specific parameters\n",
        "# ============================================================================\n",
        "task_suite_name = \"libero_spatial_displacement\"  # Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90\n",
        "num_steps_wait = 10  # Number of steps to wait for objects to stabilize in sim\n",
        "num_trials_per_task = 1  # Number of rollouts per task\n",
        "\n",
        "# ============================================================================\n",
        "# LIBERO Pro parameters\n",
        "# ============================================================================\n",
        "evaluation_config_path = \"/hdd/zijianwang/openpi/third_party/LIBERO-PRO/evaluation_config.yaml\"\n",
        "\n",
        "# ============================================================================\n",
        "# Logging and experiment tracking parameters\n",
        "# ============================================================================\n",
        "local_log_dir = \"./experiments/logs\"\n",
        "run_id_note = None\n",
        "save_experiment_config_flag = True\n",
        "\n",
        "# ============================================================================\n",
        "# Video output\n",
        "# ============================================================================\n",
        "video_out_path = \"./experiments/videos/\"\n",
        "\n",
        "# ============================================================================\n",
        "# Random seed\n",
        "# ============================================================================\n",
        "seed = 7\n",
        "\n",
        "# ============================================================================\n",
        "# Combine all args into a dictionary\n",
        "# ============================================================================\n",
        "args = {\n",
        "    'host': host,\n",
        "    'port': port,\n",
        "    'resize_size': resize_size,\n",
        "    'replan_steps': replan_steps,\n",
        "    'sampling_bs': sampling_bs,\n",
        "    'task_suite_name': task_suite_name,\n",
        "    'num_steps_wait': num_steps_wait,\n",
        "    'num_trials_per_task': num_trials_per_task,\n",
        "    'evaluation_config_path': evaluation_config_path,\n",
        "    'local_log_dir': local_log_dir,\n",
        "    'run_id_note': run_id_note,\n",
        "    'save_experiment_config': save_experiment_config_flag,\n",
        "    'video_out_path': video_out_path,\n",
        "    'seed': seed,\n",
        "}\n",
        "\n",
        "print(\"✓ Hyperparameters configured:\")\n",
        "for key, value in args.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: 运行评估\n",
        "\n",
        "这个 Cell 执行完整的评估流程。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Experiment run ID: LIBERO-PRO-libero_spatial_displacement-20251030_153314\n",
            "INFO:__main__:Log file path: ./experiments/logs/LIBERO-PRO-libero_spatial_displacement-20251030_153314.txt\n",
            "INFO:root:Experiment configuration saved to: ./experiments/logs/LIBERO-PRO-libero_spatial_displacement-20251030_153314_config.json\n",
            "INFO:root:Task suite: libero_spatial_displacement\n",
            "INFO:root:Waiting for server at ws://0.0.0.0:8001...\n",
            "INFO:__main__:Starting evaluation of task suite: libero_spatial_displacement\n",
            "INFO:__main__:Number of tasks: 10\n",
            "INFO:__main__:Trials per task: 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] Using default task order for benchmark 'libero_spatial_displacement' (10 tasks).\n",
            "✓ Initialization complete, starting task loop...\n"
          ]
        }
      ],
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger, run_id, log_filepath = setup_logging(args)\n",
        "\n",
        "# Save experiment configuration\n",
        "save_experiment_config(args, run_id, log_filepath)\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(args['seed'])\n",
        "\n",
        "# # Initialize environment perturbation for LIBERO Pro\n",
        "# with open(args['evaluation_config_path']) as f:\n",
        "#     evaluation_cfg = yaml.safe_load(f)\n",
        "\n",
        "# evaluation_cfg[\"bddl_files_path\"] = evaluation_cfg.get(\"bddl_files_path\", \"\") + \"/\" + args['task_suite_name']\n",
        "# evaluation_cfg[\"task_suite_name\"] = args['task_suite_name']\n",
        "\n",
        "# if not os.path.exists(evaluation_cfg.get(\"init_file_dir\", \"\") + args['task_suite_name'] + \"_temp/\"):\n",
        "#     perturbation.create_env(configs=evaluation_cfg)\n",
        "\n",
        "# Initialize LIBERO task suite\n",
        "benchmark_dict = benchmark.get_benchmark_dict()\n",
        "task_suite = benchmark_dict[args['task_suite_name']]()\n",
        "num_tasks_in_suite = task_suite.n_tasks\n",
        "logging.info(f\"Task suite: {args['task_suite_name']}\")\n",
        "\n",
        "pathlib.Path(args['video_out_path']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Determine max_steps based on task suite\n",
        "if \"libero_spatial\" in args['task_suite_name']:\n",
        "    max_steps = 220\n",
        "elif \"libero_object\" in args['task_suite_name']:\n",
        "    max_steps = 280\n",
        "elif \"libero_goal\" in args['task_suite_name']:\n",
        "    max_steps = 300\n",
        "elif \"libero_10\" in args['task_suite_name']:\n",
        "    max_steps = 520\n",
        "elif \"libero_90\" in args['task_suite_name']:\n",
        "    max_steps = 400\n",
        "else:\n",
        "    raise ValueError(f\"Unknown task suite: {args['task_suite_name']}\")\n",
        "\n",
        "client = _websocket_client_policy.WebsocketClientPolicy(args['host'], args['port'])\n",
        "\n",
        "# Start evaluation\n",
        "total_episodes, total_successes = 0, 0\n",
        "\n",
        "logger.info(f\"Starting evaluation of task suite: {args['task_suite_name']}\")\n",
        "logger.info(f\"Number of tasks: {num_tasks_in_suite}\")\n",
        "logger.info(f\"Trials per task: {args['num_trials_per_task']}\")\n",
        "\n",
        "print(\"✓ Initialization complete, starting task loop...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Warning]: datasets path /hdd/zijianwang/openpi/third_party/LIBERO-PRO/libero/libero/../datasets does not exist!\n",
            "[Warning]: datasets path /hdd/zijianwang/openpi/third_party/LIBERO-PRO/libero/libero/../datasets does not exist!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\n",
            "Starting task: pick up the black bowl next to the plate and place it on the plate\n",
            "INFO:__main__:Building reusable ValueMap...\n",
            "INFO:root:可重复使用ValueMap构建完成:\n",
            "INFO:root:  目标对象: ['akita_black_bowl_1']\n",
            "INFO:root:  避免对象: ['akita_black_bowl_2', 'cookies_1', 'glazed_rim_porcelain_ramekin_1', 'plate_1', 'wooden_cabinet_1', 'flat_stove_1']\n",
            "INFO:root:  地图大小: 100\n",
            "INFO:root:  分辨率: [0.011  0.016  0.0105]\n",
            "INFO:root:  当前末端执行器位置: [-0.20846466  0.          1.17327948]\n",
            "INFO:root:已恢复原始环境状态\n",
            "INFO:__main__:Reusable ValueMap built successfully:\n",
            "INFO:__main__:  Target objects: ['akita_black_bowl_1']\n",
            "INFO:__main__:  Avoid objects: ['akita_black_bowl_2', 'cookies_1', 'glazed_rim_porcelain_ramekin_1', 'plate_1', 'wooden_cabinet_1', 'flat_stove_1']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "## voxel resolution: [0.011  0.016  0.0105]\n",
            "##################################################\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\n",
            "Task: pick up the black bowl next to the plate and place it on the plate\n",
            "INFO:__main__:Episode 1/1\n",
            "INFO:__main__:Starting episode 1...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "** saving visualization to experiments/tmp/visualizations/16:9:35.html ...\n",
            "** saving visualization to experiments/tmp/visualizations/latest.html ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Step 10 - Best traj cost: [0.23895795 0.23895795 0.2362535  0.22659561 0.22659561]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "** save to experiments/tmp/visualizations/16:9:35.html\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [05:35<?, ?it/s]\n",
            " 80%|████████  | 8/10 [05:44<01:26, 43.03s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m batch_eef_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(action_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 110\u001b[0m     eef_traj \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_eef_trajectory_from_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_chunk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     batch_eef_positions\u001b[38;5;241m.\u001b[39mappend(eef_traj)\n\u001b[1;32m    112\u001b[0m eef_trajs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(batch_eef_positions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/examples/libero/util.py:28\u001b[0m, in \u001b[0;36mcompute_eef_trajectory_from_actions\u001b[0;34m(env, action_chunk)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m action_chunk:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# 直接使用env.step()获取最准确的位置\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     30\u001b[0m             env\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 重置done标志\u001b[39;00m\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/third_party/LIBERO-PRO/libero/libero/envs/env_wrapper.py:241\u001b[0m, in \u001b[0;36mControlEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 241\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_obs \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, done, info\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/third_party/LIBERO-PRO/libero/libero/envs/bddl_base_domain.py:807\u001b[0m, in \u001b[0;36mBDDLBaseDomain.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    804\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action)\n\u001b[1;32m    805\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((action[:\u001b[38;5;241m3\u001b[39m], action[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 807\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_success()\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, done, info\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/examples/libero/.venv/lib/python3.8/site-packages/robosuite/environments/base.py:393\u001b[0m, in \u001b[0;36mMujocoEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol_timestep \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_timestep)):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mforward()\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pre_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_observables()\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/third_party/LIBERO-PRO/libero/libero/envs/bddl_base_domain.py:813\u001b[0m, in \u001b[0;36mBDDLBaseDomain._pre_action\u001b[0;34m(self, action, policy_step)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pre_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, action, policy_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pre_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_step\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/examples/libero/.venv/lib/python3.8/site-packages/robosuite/environments/robot_env.py:583\u001b[0m, in \u001b[0;36mRobotEnv._pre_action\u001b[0;34m(self, action, policy_step)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, robot \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobots):\n\u001b[1;32m    582\u001b[0m     robot_action \u001b[38;5;241m=\u001b[39m action[cutoff : cutoff \u001b[38;5;241m+\u001b[39m robot\u001b[38;5;241m.\u001b[39maction_dim]\n\u001b[0;32m--> 583\u001b[0m     \u001b[43mrobot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrobot_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m     cutoff \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m robot\u001b[38;5;241m.\u001b[39maction_dim\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/examples/libero/.venv/lib/python3.8/site-packages/robosuite/robots/single_arm.py:250\u001b[0m, in \u001b[0;36mSingleArm.control\u001b[0;34m(self, action, policy_step)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontroller\u001b[38;5;241m.\u001b[39mset_goal(arm_action)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Now run the controller for a step\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m torques \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Clip the torques\u001b[39;00m\n\u001b[1;32m    253\u001b[0m low, high \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorque_limits\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/examples/libero/.venv/lib/python3.8/site-packages/robosuite/controllers/osc.py:291\u001b[0m, in \u001b[0;36mOperationalSpaceController.run_controller\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03mCalculates the torques required to reach the desired setpoint.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m     np.array: Command torques\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Update state\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m desired_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Only linear interpolator is currently supported\u001b[39;00m\n",
            "File \u001b[0;32m/hdd/zijianwang/openpi/examples/libero/.venv/lib/python3.8/site-packages/robosuite/controllers/base_controller.py:151\u001b[0m, in \u001b[0;36mController.update\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqpos[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqpos_index])\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_vel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqvel[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqvel_index])\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_site_jacp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meef_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqvel_index\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_ori \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget_site_jacr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meef_name)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqvel_index])\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_full \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mvstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_pos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_ori]))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Get current timestamp for this run\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "video_out_dir = pathlib.Path(args['video_out_path']) / timestamp / args['task_suite_name']\n",
        "video_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "for task_id in tqdm.tqdm(range(num_tasks_in_suite)):\n",
        "    if task_id != 8:\n",
        "        continue\n",
        "    \n",
        "    # Get task\n",
        "    task = task_suite.get_task(task_id)\n",
        "    initial_states = task_suite.get_task_init_states(task_id)\n",
        "    env, task_description = _get_libero_env(task, LIBERO_ENV_RESOLUTION, args['seed'])\n",
        "\n",
        "    # Start episodes\n",
        "    task_episodes, task_successes = 0, 0\n",
        "    logger.info(f\"\\nStarting task: {task_description}\")\n",
        "\n",
        "    all_episode_costs = []\n",
        "    current_episode_costs = []\n",
        "\n",
        "    ### Build reusable valuemap before task starts\n",
        "    logger.info(\"Building reusable ValueMap...\")\n",
        "    reusable_valuemap = None\n",
        "    try:\n",
        "        env.reset()\n",
        "        obs = env.set_init_state(initial_states[0])\n",
        "        reusable_valuemap = build_reusable_value_map(env, task_description)\n",
        "\n",
        "        if \"error\" not in reusable_valuemap:\n",
        "            logger.info(f\"Reusable ValueMap built successfully:\")\n",
        "            logger.info(f\"  Target objects: {reusable_valuemap.get('target_objects', [])}\")\n",
        "            logger.info(f\"  Avoid objects: {reusable_valuemap.get('avoid_objects', [])}\")\n",
        "        else:\n",
        "            logger.warning(f\"Failed to build reusable ValueMap: {reusable_valuemap.get('error', 'Unknown error')}\")\n",
        "            reusable_valuemap = None\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to build reusable ValueMap: {e}\")\n",
        "        reusable_valuemap = None\n",
        "\n",
        "    # reusable_valuemap = None   \n",
        "\n",
        "    \n",
        "    ### Entering task loop\n",
        "    for episode_idx in tqdm.tqdm(range(args['num_trials_per_task'])):\n",
        "        if episode_idx != 0:\n",
        "            continue\n",
        "        logger.info(f\"\\nTask: {task_description}\")\n",
        "        logger.info(f\"Episode {episode_idx + 1}/{args['num_trials_per_task']}\")\n",
        "\n",
        "        # Reset environment\n",
        "        env.reset()\n",
        "        robot_instance = env.robots[0]\n",
        "        action_plan = collections.deque()\n",
        "        obs = env.set_init_state(initial_states[episode_idx])\n",
        "\n",
        "        # Setup\n",
        "        t = 0\n",
        "        replay_images = []\n",
        "        current_episode_costs = []\n",
        "\n",
        "        logger.info(f\"Starting episode {task_episodes + 1}...\")\n",
        "        while t < max_steps + args['num_steps_wait']:\n",
        "            # IMPORTANT: Do nothing for the first few timesteps because the simulator drops objects\n",
        "            if t < args['num_steps_wait']:\n",
        "                obs, reward, done, info = env.step(LIBERO_DUMMY_ACTION)\n",
        "                t += 1\n",
        "                continue\n",
        "\n",
        "            # Get preprocessed image - note the 180 degree rotation\n",
        "            img = np.ascontiguousarray(obs[\"agentview_image\"][::-1, ::-1])\n",
        "            wrist_img = np.ascontiguousarray(obs[\"robot0_eye_in_hand_image\"][::-1, ::-1])\n",
        "            img = image_tools.convert_to_uint8(image_tools.resize_with_pad(img, args['resize_size'], args['resize_size']))\n",
        "            wrist_img = image_tools.convert_to_uint8(\n",
        "                image_tools.resize_with_pad(wrist_img, args['resize_size'], args['resize_size'])\n",
        "            )\n",
        "\n",
        "            # Image.fromarray(np.uint8(img)).save(\"./experiments/tmp/live_image.png\")\n",
        "            replay_images.append(img)\n",
        "\n",
        "            if not action_plan:\n",
        "                # Finished executing previous action chunk -- compute new chunk\n",
        "                element = {\n",
        "                    \"observation/image\": img,\n",
        "                    \"observation/wrist_image\": wrist_img,\n",
        "                    \"observation/state\": np.concatenate(\n",
        "                        (\n",
        "                            obs[\"robot0_eef_pos\"],\n",
        "                                _quat2axisangle(obs[\"robot0_eef_quat\"]),\n",
        "                            obs[\"robot0_gripper_qpos\"],\n",
        "                        )\n",
        "                    ),\n",
        "                    \"prompt\": str(task_description),\n",
        "                    \"sampling_bs\": int(args['sampling_bs']),\n",
        "                }\n",
        "\n",
        "                # Query model to get action\n",
        "                action_chunk = client.infer(element)[\"actions\"]\n",
        "                assert action_chunk.shape[-2] >= args['replan_steps'], (\n",
        "                    f\"We want to replan every {args['replan_steps']} steps, but policy only predicts {action_chunk.shape[-2]} steps.\"\n",
        "                )\n",
        "\n",
        "                # Evaluate trajectory using built valuemap\n",
        "                try:\n",
        "                    if action_chunk.ndim == 2:\n",
        "                        action_chunk = np.expand_dims(action_chunk, axis=0)\n",
        "\n",
        "                    batch_eef_positions = []\n",
        "                    for i in range(action_chunk.shape[0]):\n",
        "                        eef_traj = compute_eef_trajectory_from_actions(env, action_chunk[i])\n",
        "                        batch_eef_positions.append(eef_traj)\n",
        "                    eef_trajs = np.stack(batch_eef_positions, axis=0)\n",
        "\n",
        "                    if reusable_valuemap is not None:\n",
        "                        evaluation_result = evaluate_trajectory_with_value_map(\n",
        "                            reusable_valuemap, eef_trajs, current_env=env\n",
        "                        )\n",
        "                        \n",
        "                        traj_cost = evaluation_result[\"step_info\"][\"traj_cost\"] # shape: (num_traj, num_steps)\n",
        "                        best_traj_id = evaluation_result[\"step_info\"][\"best_traj_id\"]\n",
        "                        best_action_chunk = action_chunk[best_traj_id]\n",
        "                        best_traj_cost = traj_cost[best_traj_id, : args['replan_steps']]\n",
        "\n",
        "                        # Record cost data for current step\n",
        "                        step_cost_data = {\n",
        "                            \"step\": t,\n",
        "                            \"best_traj_cost\": best_traj_cost.copy(),\n",
        "                            \"best_traj_id\": best_traj_id,\n",
        "                            \"replan_steps\": args['replan_steps'],\n",
        "                        }\n",
        "                        current_episode_costs.append(step_cost_data)\n",
        "                        logger.info(f\"Step {t} - Best traj cost: {best_traj_cost}\")\n",
        "                    else:\n",
        "                        logger.warning(\"No available valuemap, skipping trajectory evaluation\")\n",
        "                        best_action_chunk = action_chunk[0]\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Trajectory evaluation failed: {e}\")\n",
        "                    best_action_chunk = action_chunk[0]\n",
        "\n",
        "                action_plan.extend(best_action_chunk[: args['replan_steps']])\n",
        "\n",
        "            action = action_plan.popleft()\n",
        "\n",
        "            # Execute action in environment\n",
        "            obs, reward, done, info = env.step(action.tolist())\n",
        "            gripper_is_closed_result = is_gripper_closed(obs)\n",
        "            if done:\n",
        "                task_successes += 1\n",
        "                total_successes += 1\n",
        "                break\n",
        "            t += 1\n",
        "\n",
        "        task_episodes += 1\n",
        "        total_episodes += 1\n",
        "\n",
        "        # Save cost data for current episode\n",
        "        episode_data = {\n",
        "            \"episode_idx\": episode_idx,\n",
        "            \"task_description\": task_description,\n",
        "            \"success\": done,\n",
        "            \"total_steps\": t,\n",
        "            \"costs\": current_episode_costs.copy(),\n",
        "        }\n",
        "        all_episode_costs.append(episode_data)\n",
        "\n",
        "        logger.info(\n",
        "            f\"Episode {episode_idx + 1} completed - Success: {done}, Steps: {t}, Cost records: {len(current_episode_costs)}\"\n",
        "        )\n",
        "\n",
        "        # Save a replay video of the episode\n",
        "        # save_episode_video(replay_images, task_description, episode_idx, done, run_id, args)\n",
        "        suffix = \"success\" if done else \"failure\"\n",
        "        task_segment = task_description.replace(\" \", \"_\")\n",
        "        video_filename = f\"task{task_id:02d}_ep{episode_idx:03d}_{task_segment}_{suffix}.mp4\"\n",
        "        video_path = video_out_dir / video_filename\n",
        "        \n",
        "        imageio.mimwrite(\n",
        "            video_path,\n",
        "            [np.asarray(x) for x in replay_images],\n",
        "            fps=24,\n",
        "        )\n",
        "\n",
        "        # Log current results\n",
        "        logger.info(f\"Episode result: {'Success' if done else 'Failure'}\")\n",
        "        logger.info(f\"Completed episodes: {total_episodes}\")\n",
        "        logger.info(f\"Successful episodes: {total_successes} ({total_successes / total_episodes * 100:.1f}%)\")\n",
        "\n",
        "    # Log final results\n",
        "    task_success_rate = float(task_successes) / float(task_episodes) if task_episodes > 0 else 0\n",
        "    total_success_rate = float(total_successes) / float(total_episodes) if total_episodes > 0 else 0\n",
        "\n",
        "    logger.info(f\"Current task success rate: {task_success_rate:.4f} ({task_success_rate * 100:.1f}%)\")\n",
        "    logger.info(f\"Overall success rate: {total_success_rate:.4f} ({total_success_rate * 100:.1f}%)\")\n",
        "    logger.info(f\"Current task episodes: {task_episodes}, successful: {task_successes}\")\n",
        "    logger.info(f\"Total episodes: {total_episodes}, total successful: {total_successes}\")\n",
        "\n",
        "    # Save cost data for current task\n",
        "    task_cost_file = f\"./experiments/cost/{run_id}/cost_data_task_{task_id}.json\"\n",
        "    os.makedirs(os.path.dirname(task_cost_file), exist_ok=True)\n",
        "    with open(task_cost_file, \"w\") as f:\n",
        "        json.dump(all_episode_costs, f, indent=2, default=str)\n",
        "    logger.info(f\"Cost data saved to: {task_cost_file}\")\n",
        "\n",
        "    break\n",
        "\n",
        "# Calculate final results\n",
        "final_success_rate = float(total_successes) / float(total_episodes) if total_episodes > 0 else 0\n",
        "\n",
        "# Log final results\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"Experiment completed - Final results:\")\n",
        "logger.info(f\"Total episodes: {total_episodes}\")\n",
        "logger.info(f\"Total successful: {total_successes}\")\n",
        "logger.info(f\"Final success rate: {final_success_rate:.4f} ({final_success_rate * 100:.1f}%)\")\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(f\"Experiment run ID: {run_id}\")\n",
        "logger.info(f\"Log file: {log_filepath}\")\n",
        "logger.info(\"Experiment completed!\")\n",
        "\n",
        "print(\"\\n✓ Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: 评估总结\n",
        "\n",
        "显示评估结果和输出位置。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nConfiguration Summary:\")\n",
        "print(f\"  Task Suite: {args['task_suite_name']}\")\n",
        "print(f\"  Number of Trials per Task: {args['num_trials_per_task']}\")\n",
        "print(f\"  Model Server: {args['host']}:{args['port']}\")\n",
        "print(f\"  Image Resize Size: {args['resize_size']}\")\n",
        "print(f\"  Replan Steps: {args['replan_steps']}\")\n",
        "print(f\"\\nOutput Locations:\")\n",
        "print(f\"  Log Directory: {args['local_log_dir']}\")\n",
        "print(f\"  Video Output: {args['video_out_path']}\")\n",
        "print(f\"  Cost Output: ./experiments/cost/\")\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Final Success Rate: {final_success_rate:.4f} ({final_success_rate * 100:.1f}%)\")\n",
        "print(f\"  Total Episodes: {total_episodes}\")\n",
        "print(f\"  Total Successes: {total_successes}\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
